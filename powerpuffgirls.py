# -*- coding: utf-8 -*-
"""PowerpuffGirls.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bOMC-S0BeBM7T_HCOVGIMeYUu8ngEYsk
"""

!pip install numpy
!pip install pandas
!pip install tensorflow

import numpy as np
import pandas as pd
import os
import re
import string
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding, GRU, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""# Dosya yolunu kontrol etme ve yazdırma"""

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# Veri setini yükleme"""

df = pd.read_csv("/content/turkish_movie_sentiment_dataset.csv")

"""# Yorumları temizleme fonksiyonu"""

def clean_comment(comment):
    comment = comment.lower()
    comment = remove_punctuation(comment)
    comment = comment.replace("\r", " ").replace("\n", " ")
    comment = remove_numeric(comment)
    return comment

"""# Noktalama işaretlerini kaldırma fonksiyonu"""

def remove_punctuation(text):
    no_punc = [char for char in text if char not in string.punctuation]
    word_wo_punc = "".join(no_punc)
    return word_wo_punc

"""# Sayıları kaldırma fonksiyonu"""

def remove_numeric(text):
    output = "".join([char for char in text if not char.isdigit()])
    return output

"""# Yorumları ve puanları temizleme"""

comments = lambda x : x[23:-24]
df["comment"] = df["comment"].apply(comments)
floatize = lambda x : float(x[0:-2])
df["point"] = df["point"].apply(floatize)

"""
# Puanları ikili sınıfa dönüştürme"""

df.drop(df[df["point"] == 3].index, inplace=True)
df["point"] = df["point"].replace([1, 2], 0)
df["point"] = df["point"].replace([4, 5], 1)

"""# Yorumları temizleme"""

df["comment"] = df["comment"].apply(clean_comment)

"""# Veri setini eğitim ve test olarak ayırma"""

target = df["point"].values.tolist()
data = df["comment"].values.tolist()

cutoff = int(len(data) * 0.80)
X_train, X_test = data[:cutoff], data[cutoff:]
y_train, y_test = target[:cutoff], target[cutoff:]

"""
# Tokenizer ile kelime indekslerini oluşturma"""

num_words = 10000
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(data)
X_train_tokens = tokenizer.texts_to_sequences(X_train)
X_test_tokens = tokenizer.texts_to_sequences(X_test)

"""# Padding işlemi"""

num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens]
num_tokens = np.array(num_tokens)
max_tokens = int(np.mean(num_tokens) + 2 * np.std(num_tokens))
X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens)
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens)

"""# Modeli oluşturma ve eğitme"""

embedding_size = 50
model = Sequential([
    Embedding(input_dim=num_words, output_dim=embedding_size, input_length=max_tokens, name="embedding_layer"),
    GRU(units=16, return_sequences=True),
    GRU(units=8, return_sequences=True),
    GRU(units=4),
    Dense(1, activation="sigmoid")
])
optimizer = Adam(learning_rate=1e-3)
model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
model.summary()

X_train_pad = np.array(X_train_pad)
y_train = np.array(y_train)
model.fit(X_train_pad, y_train, epochs=5, batch_size=256)

"""# Yorumun duygu analizini yapma fonksiyonu"""

def sentiment_analysis_single_comment(comment):
    comment = clean_comment(comment)
    tokens = tokenizer.texts_to_sequences([comment])
    tokens_pad = pad_sequences(tokens, maxlen=max_tokens)
    prediction = model.predict(tokens_pad)[0][0]
    sentiment = "Pozitif" if prediction > 0.5 else "Negatif"
    return sentiment, prediction

"""# Tüm yorumların duygu analizini yapma fonksiyonu"""

def sentiment_analysis_all_comments(data):
    sentiments = []
    for comment in data:
        sentiment, prediction = sentiment_analysis_single_comment(comment)
        sentiments.append(sentiment)
    positive_comments = sentiments.count("Pozitif")
    negative_comments = sentiments.count("Negatif")
    return positive_comments, negative_comments

"""# Seçilen bir yorumu analiz et"""

selected_comment = X_test[15]
sentiment, prediction = sentiment_analysis_single_comment(selected_comment)
print(f"Seçilen yorum: {selected_comment}")
print(f"Duygu analizi sonucu: {sentiment} (Tahmin: {prediction})")

"""# Genel olarak tüm yorumları analiz et"""

positive_comments, negative_comments = sentiment_analysis_all_comments(X_test)
print(f"Pozitif yorum sayısı: {positive_comments}")
print(f"Negatif yorum sayısı: {negative_comments}")

"""# En sık kullanılan kelimeleri görselleştirme"""

all_comments = " ".join(data)
wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(all_comments)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# En sık kullanılan 10 kelimeyi listeleme"""

word_counts = Counter(all_comments.split())
common_words = word_counts.most_common(10)
print("En sık kullanılan 10 kelime:", common_words)

"""# Kullanıcıdan bir film adı girişi alalım"""

film_name = input("Analiz yapmak istediğiniz filmin adını girin: ")

"""# Sütun adlarını kontrol etme"""

print(df.columns)

"""# Seçilen film için yorumları filtreleyelim"""

film_yorumlari = df[df["film_name"] == film_name]

"""# Eğer seçilen film için yorum yoksa bir hata mesajı verelim"""

if film_yorumlari.empty:
    print(f"{film_name} için yorum bulunamadı.")
else:
    # Yorumları ve puanları alalım
    film_yorumlari_text = film_yorumlari["comment"].values
    film_yorumlari_point = film_yorumlari["point"].values

"""# Yorumları tokenize edelim"""

film_yorumlari_tokens = tokenizer.texts_to_sequences(film_yorumlari_text)
    film_yorumlari_pad = pad_sequences(film_yorumlari_tokens, maxlen=max_tokens)

"""# Modeli kullanarak tahmin yapalım"""

film_yorumlari_pred = model.predict(film_yorumlari_pad)
    film_yorumlari_pred = film_yorumlari_pred.T[0]
    film_yorumlari_pred_cls = np.array([1.0 if p > 0.5 else 0.0 for p in film_yorumlari_pred])

"""# Pozitif ve negatif yorumları sayalım"""

pozitif_yorum_sayisi = np.sum(film_yorumlari_pred_cls == 1.0)
    negatif_yorum_sayisi = np.sum(film_yorumlari_pred_cls == 0.0)

"""# Sonuçları yazdıralım"""

print(f"{film_name} için toplam {len(film_yorumlari)} yorum bulundu.")
    print(f"Pozitif yorum sayısı: {pozitif_yorum_sayisi}")
    print(f"Negatif yorum sayısı: {negatif_yorum_sayisi}")

if pozitif_yorum_sayisi > negatif_yorum_sayisi:
        print(f"{film_name} genel olarak iyi bir film.")
    else:
        print(f"{film_name} genel olarak kötü bir film.")